{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](flow.png)\n",
    "\n",
    "reference : Ayush Pant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the machine learning workflow\n",
    "    \n",
    "    We can define the machine learning workflow in 3 stages.\n",
    "        \n",
    "        > Gathering data\n",
    "        > Data pre-processing\n",
    "        > Researching the model that will be best for the type of data\n",
    "        > Training and testing the model\n",
    "        > Evaluation\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the machine learning Model?\n",
    "    The machine learning model is nothing but a piece of code; an engineer or data scientist makes it smart through training with data. So, if you give garbage to the model, you will get garbage in return, i.e. the trained model will provide false or wrong predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gathering Data\n",
    "    The process of gathering data depends on the type of project we desire to make, if we want to make an ML project that uses real-time data. The data set can be collected from various sources such as a file, database, sensor and many other such sources but the collected data cannot be used directly for performing the analysis process as there might be a lot of missing data, extremely large values, unorganized text data or noisy data. Therefore, to solve this problem Data Preparation is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data pre-processing\n",
    "    Data pre-processing is one of the most important steps in machine learning. It is the most important step that helps in building machine learning models more accurately. In machine learning, there is an 80/20 rule. Every data scientist should spend 80% time for data pre-processing and 20% time to actually perform the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is data pre-processing?\n",
    "    Data pre-processing is a process of cleaning the raw data i.e. the data is collected in the real world and is converted to a clean data set. In other words, whenever the data is gathered from different sources it is collected in a raw format and this data isn’t feasible for the analysis.\n",
    "    Therefore, certain steps are executed to convert the data into a small clean data set, this part of the process is called as data pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we need it?\n",
    "    As we know that data pre-processing is a process of cleaning the raw data into clean data, so that can be used to train the model. So, we definitely need data pre-processing to achieve good results from the applied model in machine learning and deep learning projects.\n",
    "    Most of the real-world data is messy, some of these types of data are:\n",
    "        1. Missing data: Missing data can be found when it is not continuously created.\n",
    "        2. Noisy data: This type of data is also called outliers, this can occur due to human errors (human manually gathering the data) or some technical problem of the device at the time of collection of data.\n",
    "        3. Inconsistent data: This type of data might be collected due to human errors (mistakes with the name or values) or duplication of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Three Types of Data\n",
    "\n",
    "    1. Numeric e.g. income, age\n",
    "    2. Categorical e.g. gender, nationality\n",
    "    3. Ordinal e.g. low/medium/high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can data pre-processing be performed?\n",
    "\n",
    "    These are some of the basic pre — processing techniques that can be used to convert raw data.\n",
    "    \n",
    "        1. Conversion of data: As we know that Machine Learning models can only handle numeric features, hence categorical and ordinal data must be somehow converted into numeric features.\n",
    "        \n",
    "        2. Ignoring the missing values: Whenever we encounter missing data in the data set then we can remove the row or column of data depending on our need. This method is known to be efficient but it shouldn’t be performed if there are a lot of missing values in the dataset.\n",
    "        \n",
    "        3. Filling the missing values: Whenever we encounter missing data in the data set then we can fill the missing data manually, most commonly the mean, median or highest frequency value is used.\n",
    "        \n",
    "        4. Machine learning: If we have some missing data then we can predict what data shall be present at the empty position by using the existing data.\n",
    "        \n",
    "        5. Outliers detection: There are some error data that might be present in our data set that deviates drastically from other observations in a data set. [Example: human weight = 800 Kg; due to mistyping of extra 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Researching the model that will be best for the type of data\n",
    "\n",
    "    Our main goal is to train the best performing model possible, using the pre-processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning:\n",
    "\n",
    "    In Supervised learning, an AI system is presented with data which is labelled, which means that each data tagged with the correct label.\n",
    "    The supervised learning is categorized into 2 other categories which are “Classification” and “Regression”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification:\n",
    "\n",
    "    Classification problem is when the target variable is categorical (i.e. the output could be classified into classes — it belongs to either Class A or B or something else).\n",
    "    A classification problem is when the output variable is a category, such as “red” or “blue” , “disease” or “no disease” or “spam” or “not spam”.\n",
    "    \n",
    "![](classify.gif)    \n",
    "\n",
    "    As shown in the above representation, we have 2 classes which are plotted on the graph i.e. red and blue which can be represented as ‘setosa flower’ and ‘versicolor flower’, we can image the X-axis as ther ‘Sepal Width’ and the Y-axis as the ‘Sepal Length’, so we try to create the best fit line that separates both classes of flowers.\n",
    "    \n",
    "    These some most used classification algorithms.\n",
    "        K-Nearest Neighbor\n",
    "        Naive Bayes\n",
    "        Decision Trees/Random Forest\n",
    "        Support Vector Machine\n",
    "        Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression:\n",
    "    While a Regression problem is when the target variable is continuous (i.e. the output is numeric).\n",
    "    \n",
    "        These some most used regression algorithms.\n",
    "        \n",
    "        Linear Regression\n",
    "        Support Vector Regression\n",
    "        Decision Tress/Random Forest\n",
    "        Gaussian Progresses Regression\n",
    "        Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning:\n",
    "\n",
    "    In unsupervised learning, an AI system is presented with unlabeled, un-categorized data and the system’s algorithms act on the data without prior training. The output is dependent upon the coded algorithms. \n",
    "\n",
    "![](cluster.jpg)\n",
    "\n",
    "    The unsupervised learning is categorized into 2 other categories which are “Clustering” and “Association”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering:\n",
    "    \n",
    "    A set of inputs is to be divided into groups. Unlike in classification, the groups are not known beforehand, making this typically an unsupervised task.\n",
    "    \n",
    "![](cluster2.png)    \n",
    "\n",
    "    Methods used for clustering are:\n",
    "    \n",
    "        Gaussian mixtures\n",
    "        K-Means Clustering\n",
    "        Boosting\n",
    "        Hierarchical Clustering\n",
    "        K-Means Clustering\n",
    "        Spectral Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training and testing the model on data\n",
    "\n",
    "    For training a model we initially split the model into 3 three sections which are ‘Training data’ ,‘Validation data’ and ‘Testing data’.\n",
    "    You train the classifier using ‘training data set’, tune the parameters using ‘validation set’ and then test the performance of your classifier on unseen ‘test data set’. An important point to note is that during training the classifier only the training and/or validation set is available. The test data set must not be used during training the classifier. The test set will only be available during testing the classifier.\n",
    "    \n",
    "    Training set: The training set is the material through which the computer learns how to process information. Machine learning uses algorithms to perform the training part. A set of data used for learning, that is to fit the parameters of the classifier.\n",
    "    \n",
    "    Validation set: Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. A set of unseen data is used from the training data to tune the parameters of a classifier.\n",
    "    \n",
    "    Test set: A set of unseen data used only to assess the performance of a fully-specified classifier.\n",
    "    \n",
    "    Once the data is divided into the 3 given segments we can start the training process.\n",
    "    \n",
    "        In a data set, a training set is implemented to build up a model, while a test (or validation) set is to validate the model built. Data points in the training set are excluded from the test (validation) set. Usually, a data set is divided into a training set, a validation set (some people use ‘test set’ instead) in each iteration, or divided into a training set, a validation set and a test set in each iteration.\n",
    "        \n",
    "        The model uses any one of the models that we had chosen in step 3/ point 3. Once the model is trained we can use the same trained model to predict using the testing data i.e. the unseen data. Once this is done we can develop a confusion matrix, this tells us how well our model is trained. A confusion matrix has 4 parameters, which are ‘True positives’, ‘True Negatives’, ‘False Positives’ and ‘False Negative’. We prefer that we get more values in the True negatives and true positives to get a more accurate model. The size of the Confusion matrix completely depends upon the number of classes.\n",
    "        \n",
    "![](confusion.png)\n",
    "\n",
    "    True positives : These are cases in which we predicted TRUE and our predicted output is correct.\n",
    "    True negatives : We predicted FALSE and our predicted output is correct.\n",
    "    False positives : We predicted TRUE, but the actual predicted output is FALSE.\n",
    "    False negatives : We predicted FALSE, but the actual predicted output is TRUE.\n",
    "    \n",
    "    We can also find out the accuracy of the model using the confusion matrix.\n",
    "    \n",
    "    Accuracy = (True Positives +True Negatives) / (Total number of classes)\n",
    "        \n",
    "        i.e. for the above example:\n",
    "        \n",
    "    Accuracy = (100 + 50) / 165 = 0.9090 (90.9% accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluation\n",
    "    Model Evaluation is an integral part of the model development process. It helps to find the best model that represents our data and how well the chosen model will work in the future.\n",
    "    \n",
    "    To improve the model we might tune the hyper-parameters of the model and try to improve the accuracy and also looking at the confusion matrix to try to increase the number of true positives and true negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
